{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"./datasets/pre_processed_dataset.csv\"\n",
    "names = [\"tweet\"]\n",
    "data = pd.read_csv(url,names=names)\n",
    "data = data.drop(data.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "sentences = data.tweet\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "data_set_tokenized = []\n",
    "\n",
    "for i in sentences:\n",
    "    word_tokens = word_tokenize(i)\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    data_set_tokenized.append(filtered_sentence)\n",
    "# filtered_sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec with skipgramc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class callback(CallbackAny2Vec):\n",
    "    '''Callback to print loss after each epoch.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.get_latest_training_loss = 0\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss() - self.get_latest_training_loss\n",
    "        self.get_latest_training_loss = model.get_latest_training_loss()\n",
    "        print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
    "        self.epoch += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 3456.19384765625\n",
      "Loss after epoch 1: 3466.2734375\n",
      "Loss after epoch 2: 3462.94287109375\n",
      "Loss after epoch 3: 3310.583984375\n",
      "Loss after epoch 4: 3521.51953125\n",
      "Loss after epoch 5: 3466.419921875\n",
      "Loss after epoch 6: 3604.15625\n",
      "Loss after epoch 7: 3341.4375\n",
      "Loss after epoch 8: 3385.228515625\n",
      "Loss after epoch 9: 3567.513671875\n",
      "Loss after epoch 10: 3133.34375\n",
      "Loss after epoch 11: 3023.7734375\n",
      "Loss after epoch 12: 2946.80859375\n",
      "Loss after epoch 13: 2721.5078125\n",
      "Loss after epoch 14: 2659.58203125\n",
      "Loss after epoch 15: 2718.29296875\n",
      "Loss after epoch 16: 2428.05078125\n",
      "Loss after epoch 17: 2391.83203125\n",
      "Loss after epoch 18: 2318.3046875\n",
      "Loss after epoch 19: 2405.046875\n",
      "Loss after epoch 20: 2153.23046875\n",
      "Loss after epoch 21: 2216.98828125\n",
      "Loss after epoch 22: 2357.921875\n",
      "Loss after epoch 23: 2370.359375\n",
      "Loss after epoch 24: 2359.6015625\n",
      "Loss after epoch 25: 2192.328125\n",
      "Loss after epoch 26: 2423.921875\n",
      "Loss after epoch 27: 2312.1484375\n",
      "Loss after epoch 28: 2195.2109375\n",
      "Loss after epoch 29: 2165.2109375\n",
      "Loss after epoch 30: 2411.9296875\n",
      "Loss after epoch 31: 2113.3984375\n",
      "Loss after epoch 32: 2214.40625\n",
      "Loss after epoch 33: 2342.9375\n",
      "Loss after epoch 34: 2182.359375\n",
      "Loss after epoch 35: 2322.9375\n",
      "Loss after epoch 36: 2411.328125\n",
      "Loss after epoch 37: 2085.375\n",
      "Loss after epoch 38: 2465.296875\n",
      "Loss after epoch 39: 2358.5234375\n",
      "Loss after epoch 40: 2260.9609375\n",
      "Loss after epoch 41: 2385.3515625\n",
      "Loss after epoch 42: 2452.265625\n",
      "Loss after epoch 43: 2371.8046875\n",
      "Loss after epoch 44: 2431.5859375\n",
      "Loss after epoch 45: 2242.4453125\n",
      "Loss after epoch 46: 2069.5703125\n",
      "Loss after epoch 47: 2398.09375\n",
      "Loss after epoch 48: 2106.25\n",
      "Loss after epoch 49: 2304.71875\n",
      "Loss after epoch 50: 2324.7890625\n",
      "Loss after epoch 51: 2066.953125\n",
      "Loss after epoch 52: 2103.46875\n",
      "Loss after epoch 53: 2272.6875\n",
      "Loss after epoch 54: 2212.171875\n",
      "Loss after epoch 55: 2259.6875\n",
      "Loss after epoch 56: 2418.6875\n",
      "Loss after epoch 57: 2271.390625\n",
      "Loss after epoch 58: 2088.890625\n",
      "Loss after epoch 59: 2098.90625\n",
      "Loss after epoch 60: 2093.328125\n",
      "Loss after epoch 61: 2033.578125\n",
      "Loss after epoch 62: 2104.8125\n",
      "Loss after epoch 63: 2161.625\n",
      "Loss after epoch 64: 2078.921875\n",
      "Loss after epoch 65: 2041.265625\n",
      "Loss after epoch 66: 1998.953125\n",
      "Loss after epoch 67: 2157.90625\n",
      "Loss after epoch 68: 1839.90625\n",
      "Loss after epoch 69: 2049.734375\n",
      "Loss after epoch 70: 2277.890625\n",
      "Loss after epoch 71: 1947.0\n",
      "Loss after epoch 72: 2061.21875\n",
      "Loss after epoch 73: 1940.234375\n",
      "Loss after epoch 74: 1922.265625\n",
      "Loss after epoch 75: 2006.390625\n",
      "Loss after epoch 76: 1947.859375\n",
      "Loss after epoch 77: 1937.03125\n",
      "Loss after epoch 78: 1941.25\n",
      "Loss after epoch 79: 1898.828125\n",
      "Loss after epoch 80: 2011.125\n",
      "Loss after epoch 81: 1933.046875\n",
      "Loss after epoch 82: 1925.65625\n",
      "Loss after epoch 83: 1857.484375\n",
      "Loss after epoch 84: 1849.328125\n",
      "Loss after epoch 85: 1761.796875\n",
      "Loss after epoch 86: 1915.515625\n",
      "Loss after epoch 87: 1927.828125\n",
      "Loss after epoch 88: 1874.3125\n",
      "Loss after epoch 89: 1942.5625\n",
      "Loss after epoch 90: 1927.8125\n",
      "Loss after epoch 91: 1944.140625\n",
      "Loss after epoch 92: 1781.75\n",
      "Loss after epoch 93: 1771.5625\n",
      "Loss after epoch 94: 1793.296875\n",
      "Loss after epoch 95: 1800.171875\n",
      "Loss after epoch 96: 1788.875\n",
      "Loss after epoch 97: 1853.9375\n",
      "Loss after epoch 98: 1688.203125\n",
      "Loss after epoch 99: 1779.15625\n",
      "Loss after epoch 100: 1907.375\n",
      "Loss after epoch 101: 1736.671875\n",
      "Loss after epoch 102: 1693.578125\n",
      "Loss after epoch 103: 1579.75\n",
      "Loss after epoch 104: 1636.84375\n",
      "Loss after epoch 105: 1729.515625\n",
      "Loss after epoch 106: 1768.671875\n",
      "Loss after epoch 107: 1682.109375\n",
      "Loss after epoch 108: 1452.453125\n",
      "Loss after epoch 109: 1677.171875\n",
      "Loss after epoch 110: 1885.25\n",
      "Loss after epoch 111: 1780.109375\n",
      "Loss after epoch 112: 1653.9375\n",
      "Loss after epoch 113: 1712.46875\n",
      "Loss after epoch 114: 1724.796875\n",
      "Loss after epoch 115: 1692.109375\n",
      "Loss after epoch 116: 1700.625\n",
      "Loss after epoch 117: 1682.0625\n",
      "Loss after epoch 118: 1687.609375\n",
      "Loss after epoch 119: 1611.015625\n",
      "Loss after epoch 120: 1747.0\n",
      "Loss after epoch 121: 1621.6875\n",
      "Loss after epoch 122: 1666.0625\n",
      "Loss after epoch 123: 1539.75\n",
      "Loss after epoch 124: 1722.90625\n",
      "Loss after epoch 125: 1578.65625\n",
      "Loss after epoch 126: 1503.875\n",
      "Loss after epoch 127: 1532.375\n",
      "Loss after epoch 128: 1600.6875\n",
      "Loss after epoch 129: 1734.03125\n",
      "Loss after epoch 130: 1502.625\n",
      "Loss after epoch 131: 1685.9375\n",
      "Loss after epoch 132: 1505.625\n",
      "Loss after epoch 133: 1663.6875\n",
      "Loss after epoch 134: 1485.96875\n",
      "Loss after epoch 135: 1467.59375\n",
      "Loss after epoch 136: 1545.65625\n",
      "Loss after epoch 137: 1390.46875\n",
      "Loss after epoch 138: 1560.28125\n",
      "Loss after epoch 139: 1555.4375\n",
      "Loss after epoch 140: 1585.75\n",
      "Loss after epoch 141: 1499.65625\n",
      "Loss after epoch 142: 1528.21875\n",
      "Loss after epoch 143: 1437.15625\n",
      "Loss after epoch 144: 1563.90625\n",
      "Loss after epoch 145: 1529.03125\n",
      "Loss after epoch 146: 1342.90625\n",
      "Loss after epoch 147: 1525.0625\n",
      "Loss after epoch 148: 1452.6875\n",
      "Loss after epoch 149: 1553.65625\n",
      "Loss after epoch 150: 1595.59375\n",
      "Loss after epoch 151: 1514.4375\n",
      "Loss after epoch 152: 1676.5625\n",
      "Loss after epoch 153: 1414.09375\n",
      "Loss after epoch 154: 1533.78125\n",
      "Loss after epoch 155: 1484.03125\n",
      "Loss after epoch 156: 1573.3125\n",
      "Loss after epoch 157: 1415.625\n",
      "Loss after epoch 158: 1582.5\n",
      "Loss after epoch 159: 1507.46875\n",
      "Loss after epoch 160: 1456.40625\n",
      "Loss after epoch 161: 1545.5\n",
      "Loss after epoch 162: 1418.0\n",
      "Loss after epoch 163: 1462.59375\n",
      "Loss after epoch 164: 1460.6875\n",
      "Loss after epoch 165: 1549.875\n",
      "Loss after epoch 166: 1609.59375\n",
      "Loss after epoch 167: 1520.5\n",
      "Loss after epoch 168: 1434.875\n",
      "Loss after epoch 169: 1573.21875\n",
      "Loss after epoch 170: 1417.59375\n",
      "Loss after epoch 171: 1473.03125\n",
      "Loss after epoch 172: 1414.34375\n",
      "Loss after epoch 173: 1397.875\n",
      "Loss after epoch 174: 1506.46875\n",
      "Loss after epoch 175: 1555.6875\n",
      "Loss after epoch 176: 1416.40625\n",
      "Loss after epoch 177: 1647.96875\n",
      "Loss after epoch 178: 1400.0625\n",
      "Loss after epoch 179: 1327.875\n",
      "Loss after epoch 180: 1457.71875\n",
      "Loss after epoch 181: 1693.6875\n",
      "Loss after epoch 182: 1528.15625\n",
      "Loss after epoch 183: 1411.125\n",
      "Loss after epoch 184: 1604.78125\n",
      "Loss after epoch 185: 1558.59375\n",
      "Loss after epoch 186: 1373.78125\n",
      "Loss after epoch 187: 1365.875\n",
      "Loss after epoch 188: 1443.96875\n",
      "Loss after epoch 189: 1321.5625\n",
      "Loss after epoch 190: 1406.53125\n",
      "Loss after epoch 191: 1477.3125\n",
      "Loss after epoch 192: 1420.4375\n",
      "Loss after epoch 193: 1521.3125\n",
      "Loss after epoch 194: 1505.6875\n",
      "Loss after epoch 195: 1617.84375\n",
      "Loss after epoch 196: 1597.09375\n",
      "Loss after epoch 197: 1319.6875\n",
      "Loss after epoch 198: 1408.84375\n",
      "Loss after epoch 199: 1418.4375\n"
     ]
    }
   ],
   "source": [
    "skip_gram_model = Word2Vec(sentences=data_set_tokenized, vector_size=50, window=2, min_count=4, workers=2, epochs=200, sg=1,\n",
    "        compute_loss=True, callbacks=[callback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('food', 0.47279417514801025),\n",
       " ('kenyans', 0.4098816514015198),\n",
       " ('us', 0.40324679017066956),\n",
       " ('lifting', 0.40212175250053406),\n",
       " ('longterm', 0.39845240116119385),\n",
       " ('foods', 0.39564260840415955),\n",
       " ('called', 0.3852192759513855),\n",
       " ('read', 0.3750094175338745),\n",
       " ('going', 0.3703593909740448),\n",
       " ('old', 0.3680415153503418)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_gram_model.wv.most_similar('gmo',topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('food', 0.47279417514801025),\n",
       " ('kenyans', 0.4098816514015198),\n",
       " ('us', 0.40324679017066956),\n",
       " ('lifting', 0.40212175250053406),\n",
       " ('longterm', 0.39845240116119385),\n",
       " ('foods', 0.39564260840415955),\n",
       " ('called', 0.3852192759513855),\n",
       " ('read', 0.3750094175338745),\n",
       " ('going', 0.3703593909740448),\n",
       " ('old', 0.3680415153503418)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_gram_model.wv.most_similar('gmo',topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mexico', 0.7479697465896606),\n",
       " ('american', 0.6770910620689392),\n",
       " ('report', 0.6595668792724609),\n",
       " ('effects', 0.6565154194831848),\n",
       " ('products', 0.6527459025382996),\n",
       " ('fed', 0.6449334025382996),\n",
       " ('ban', 0.6268152594566345),\n",
       " ('long', 0.6093230247497559),\n",
       " ('modified', 0.6037793159484863),\n",
       " ('rich', 0.5912718772888184)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_gram_model.wv.most_similar('corn',topn=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec with CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 2229.346923828125\n",
      "Loss after epoch 1: 2304.605712890625\n",
      "Loss after epoch 2: 2364.0224609375\n",
      "Loss after epoch 3: 2250.18994140625\n",
      "Loss after epoch 4: 2307.58984375\n"
     ]
    }
   ],
   "source": [
    "cbow_model = Word2Vec(sentences=data_set_tokenized, vector_size=100, window=10, min_count=4, workers=2, epochs=5, sg=0,\n",
    "        compute_loss=True, callbacks=[callback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('war', 0.3049982786178589),\n",
       " ('chemicals', 0.21580922603607178),\n",
       " ('labeling', 0.20013035833835602),\n",
       " ('news', 0.1575966775417328),\n",
       " ('say', 0.15510551631450653),\n",
       " ('also', 0.15459318459033966),\n",
       " ('herbicide', 0.1451404094696045),\n",
       " ('seeds', 0.1409464329481125),\n",
       " ('scientists', 0.1345074325799942),\n",
       " ('chestnut', 0.13428443670272827)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.wv.most_similar('crop',topn=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
